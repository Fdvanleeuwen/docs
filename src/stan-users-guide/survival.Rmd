# Survival Models {#survival-models.chapter}

Survival models arise when there is an event of interest for
a group of subjects that is

* certain to occur after some amount of time, 
* but only measured for a fixed period of time, during which the event
* may not have occurred for all subjects.

For example, one might wish to estimate the the distribution of time
to failure for solid state drives in a data center, but only measure
drives for a two year period, after which some number will have failed
and some will still be in service.

Survival models are often used comparatively, such as comparing time
to death of patients diagnosed with stage one liver cancer under a new
treatment and a standard treatment (pure controls are not allowed when
there is an effective existing treatment for a serious condition).
During a two year trial, some patients will die and others will survive.

Survival models may involve covariates, such as the factory at which a
component is manufactured, the day on which it is manufactured, and
the amount of usage it gets.  A clinical trial might be adjusted for
the sex and age of a cancer patient or the hospital at which treatment
is received.

## Parametric survival models

The simplest kind of survival models to understand are parametric
models in which the survival time is modeled explicitly.

### Data format

First, consider the data for a simple survival analysis without
covariates.

```stan
data {
  int<lower=0> N;
  vector[N] t;
  int<lower=0> N_cens;
  real<lower=0> t_cens;
}
```

In this program, `N` is the number of uncensored observations and `t`
contains the times of the uncensored observations.  There are a
further `N_cens` items that are right censored at time `t_cens`.
Right censoring means that if the time to failure is greater than
`t_cens`, it is only observed that the part survived until time 
`t_cens`.

### Memoryless survival models

The exponential distribution is memoryless in the sense that if $T \sim
\textrm{exponential}(\lambda)$ for some rate $\lambda > 0,$ then
\[
\Pr[T > t] = \Pr[T > t + t' \mid T > t'].
\]
If component survival times are distributed exponentially, it means the
distribution of time to failure is the same no matter how long the
item has already survived.  This can be a reasonable assumption for
electronic components, but is not a reasonable model for animal survival.

The exponential model has a single parameter for the rate, which
assumes all subjects have the same distribution of failure time. 

```stan
parameters {
  real<lower=0> lambda;
}
```

With the rate parameterization, the expected survival time is
\[
\mathbb{E}[T \mid \lambda] = \frac{1}{\lambda}.
\]
The exponential distribution is sometimes parameterized in terms of a
scale (i.e., inverse rate) $\beta = 1 / \lambda$.

The Stan model for exponential survival is as follows.

```stan
model {
  t ~ exponential(lambda);
  target += N_cens * exponential_lccdf(t_cens | lambda);

  lambda ~ lognormal(0, 1);
}
```

The likelihood for observed failures is just the exponential
distribution with rate `lambda`.  The Stan code is vectorized,
modeling each entry of the vector `t` as a having an exponential
distribution with rate `lambda`. This likelihood could have been
written as

```stan
for (n in 1:N) {
  t[n] ~ exponential(lambda);
}
```

The log likelihood for censored items is the number of censored items
times the log complementary cumulative distribution function at the
censoring time of the exponential distribution with rate
`lambda`.  The log likelihoods for the censored events could have
been added to the target log density one at a time,

```stan
for (n in 1:N)
  target += exponential_lccdf(t_cens | lambda);
```

to define the same log density, but it is much more efficient
computationally to multiply by a constant than do a handful of
sequential additions.

Recall that `exponential\_lccdf(t_cens | lambda)` is
the log probability that a failure distributed as
$T \sim \textrm{exponential}(\lambda)$ occurs after `t_cens`, because the
ccdf is defined as $1 - F_T(t)$, so the log ccdf is
\[
\log(1 - F_T(t)) = \log \Pr[T > t].
\]

The standard lognormal prior used in the Stan model is appropriate if
mean failure times are expected to be in the range of 0.1 to 10 time
units, because that is roughly the central 95% interval of
$\textrm{lognormal}(0, 1)$.  In general, priors should be chosen based
on knowledge of the problem, which is requires knowledge of mean failure
time.

## Survival with covariates

Suppose that for each of $n \in 1{:}N$ items observed, both censored
and uncensored, there is a covariate (row) vector $X_n \in
\mathbb{R}^K.$ For example, a clinical trial may include the age (or a
one-hot encoding of an age group) and the sex of a participant; an
electronic component might include a one-hot encoding of the factory
at which it was manufactured and a covariate for the load under which
it has been run.

Survival with covariates replaces what is essentially a simple
regression with only an intercept $\lambda$ with a generalized linear
model with a log link, where the rate for item $n$ is
\[
\lambda_n = \exp(X_n \cdot \beta),
\]
where $\beta \in \mathtbb{R}^K$ is a $K$-vector of regression
coefficients.  Thus
\[
t_n \sim \textrm{exponential}(\lambda_n).
\]
The censored items have probability
\[
\Pr[n\textrm{-th censored}] =
\textrm{exponential\_ccdf}(t^{\textrm{cens}} \mid x^{\textrm{cens}}_n
\cdot \beta).
\]

The the covariates form an $N \times K$ data matrix, $X \in
\mathbb{R}^{N \times K}$. An intercept can be added as a column of
$X$. 

A Stan program for the exponential survival model with covariates is
as follows. 

```stan
data {
  int<lower=0> N;
  vector[N] t;
  int<lower=0> N_cens;
  real<lower=0> t_cens;
  int<lower=0> K;
  matrix[N, K] x;
  matrix[N_cens, K] x_cens;
}
parameters {
  vector[K] gamma;
}
model {
  gamma ~ normal(0, 2);

  t ~ exponential(exp(x * gamma));
  target += exponential_lccdf(t_cens | exp(x_cens * gamma));
}
```

Both the censored and uncensored likelihoods are vectorized, one in
terms of the log complementary cumulative distribution function and
one in terms of the exponential distribution.


## Hazard and survival functions

Suppose $T$ is a random variable representing a survival time, with a
smooth cumulative distribution function
\[
F_T(t) = \Pr[T \leq t],
\]
so that its probability density function is
\[
p_T(t) = \frac{\textrm{d}}{\textrm{d}t} F_T(t).
\]

The *survival function* $S(t)$ is the probability of surviving until
at least time $t$, which is just the complementary cumulative
distribution function (ccdf) of the survival random variable $T$,
\[
S(t) = 1 - F_T(t).
\]
The survival function appeared in the Stan model in the previous
section as the likelihood for items that did not fail during the
period of the experiment (i.e., the censored failure times for the
items that survived through the trial period).

The *hazard function* $h(t)$ is the instantaneous risk of not
surviving past time $t$ assuming survival until time $t$, which is
given by
\[
h(t) = \frac{p_T(t)}{S(t)} = \frac{p_T(t)}{1 - F_T(t)}.
\]
The *cumulative hazard function* $H(t)$ is defined to be the accumulated
hazard over time,
\[
H(t) = \int_0^t h(u) \, \textrm{d}u.
\]

The hazard function and survival function are related through the
differential equation
\begin{eqnarray*}
h(t) & = & -\frac{\textrm{d}}{\textrm{d}t} \log S(t).
\\
& = & -\frac{1}{S(t)} \textrm{d}{\textrm{d}t} S(t)
\\
& = & \frac{1}{S(t)} \textrm{d}{\textrm{d}t} -(1 - F_Y(t))
\\
& = & \frac{1}{S(t)} \textrm{d}{\textrm{d}t} (F_Y(t) - 1)
\\
& = & \frac{1}{S(t)} \textrm{d}{\textrm{d}t} F_Y(t)
\\
& = & \frac{p_T(t)}{S(t)}.
\end{eqnarray*}

If $T \sim \textrm{exponential}(\lambda)$ has an exponential
distribution, then its hazard function is constant,
\begin{eqnarray*}
h(t)
& = & \frac{p_T(t)}{S(t)}
\\
& = & \frac{\lambda \cdot \exp(-\lambda \cdot t)}
           {1 - F_T(t)}
\\
& = & \frac{\lambda \cdot \exp(-\lambda \cdot t)}
           {1 - (1 - \exp(-\lambda \cdot t))}
\\
& = & \frac{\lambda \cdot \exp(-\lambda \cdot t)}
           {\exp(-\lambda \cdot t)}
\\
& = & \lambda.
\]
The exponential distribution is the only distribution of survival
times with a constant hazard function.


## Semi-parametric, proportional hazards model

The exponential model is parametric in the sense that it explicitly
specifies a parametric form for the density for the distribution of
survival times.  @cox:1972 introduced a semi-parametric survival model
specified directly in terms of a hazard function $h(t)$ rather than in
terms of a distribution over survival times.  Cox's model is
semi-parametric in that it does not model the full hazard function,
instead modeling only the proportional differences in hazards among
subjects.

Let $x_n \in \mathbb{R}^K$ be a (row) vector of covariates for subject
$n$ so that the full covariate data matrix $x \in \mathbb{R}^{N \times K}.  In Cox's
model, the hazard function for subject $n$ is defined conditionally in
terms of their covariates $x_n$ and the coefficients $\gamma
\in \mathbb{R}^K$ as
\[
h(t \mid x_n, \beta) = h_0(t) \cdot \exp(x_n \cdot \gamma),
\]
where $h_0(t)$ is a shared baseline hazard function and $x_n \cdot
\gamma = \sum_{k=1}^K x_{n, k} \cdot \beta_k$ is a row vector-vector
product. 

In the semi-parametric, proportional hazards model, the baseline
hazard function $h_0(t)$ is not modeled.  This is why it is called
"semi-parametric."  Only the factor $\exp(x_n \cdot \gamma),$ which
determines how individual $n$ varies by a proportion from the baseline
hazard, is modeled.  This is why it's called "proportional hazards."


### Partial likelihood function

The proportional specification of the hazard function is insufficient
to generate random variates because the baseline hazard function
$h_0(t)$ is unknown.  On the other hand, the proportional
specification is sufficient to generate a partial likelihood that
accounts for the order of the survival times.

The hazard function $h(t \mid x_n, \beta) = h_0(t) \cdot \exp(x_n
\cdot \beta)$ for subject $n$ represents the instantaneous probability
that subject $n$ fails at time $t$ given that it has survived until
time $t.$ The probability that subject $n$ is the first to fail among
$N$ subjects is thus proportional to subject $n$'s hazard function,
\[
\Pr[n \textrm{ first to fail at time } t]
\propto h(t \mid x_n, \beta).
\]
Normalizing yields
\begin{eqnarray*}
\Pr[n \textrm{ first to fail at time } t]
& = &  \frac{h(t \mid x_n, \beta)}
            {\sum_{n' = 1}^N h(t \mid x_{n'}, \beta)}
\\
& = &  \frac{h_0(t) \cdot \exp(x_n \cdot \beta)}
            {\sum_{n' = 1}^N h_0(t) \cdot \exp(x_{n'} \cdot \beta)}
\\
& = &  \frac{\exp(x_n \cdot \beta)}
            {\sum_{n' = 1}^N \exp(x_{n'} \cdot \beta)}.
\end{eqnarray*}

Suppose there are $N$ subjects with *ordered* survival times $t_1 <
t_2 < \cdots < t_N$ and covariate (row) vectors $x_1, \ldots, x_N$.
Let $t^{\textrm{cens}}$ be the (right) censoring time and let
$N^{\textrm{obs}}$ be the largest value of $n$ such that $t_n \leq
t^{\textrm{cens}}$.  This means $N^{\textrm{obs}}$ is the number of
subjects whose failure time was observed.  The ordering is for
convenient indexing and does not cause any loss of
generality---survival times can simply be sorted into the necessary
order.

The partial likelihood for each observed subject $n \in
1{:}N^{\textrm{obs}}$ is given by
\[
\Pr[n \textrm{ first to fail among } n, n + 1, \ldots N]
= \frac{\exp(x_n \cdot \beta)}
       {\sum_{n' = n}^N \exp(x_n \cdot \beta)}.
\]
The group of items for comparison and hence the summation is over all
items, including those with observed and censored failure times.

The partial likelihood is just the product of the partial likelihoods
for the observed subjects (i.e., excluding subjects whose failure time
is censored).
\[
\Pr[\textrm{observed failures ordered } 1, \ldots, N^{\textrm{obs}} |
x, \beta]
= \prod_{n = 1}^{N^{\textrm{obs}}}
  \frac{\exp(x_n \cdot \beta)}
       {\sum_{n' = n}^N \exp(x_{n'} \cdot \beta)}.
\]
On the log scale,
\begin{eqnarray*}
\log \Pr[\textrm{observed failures ordered } 1, \ldots, N^{\textrm{obs}} |
x, \beta]
& = &
\sum_{n = 1}^{N^{\textrm{obs}}}
  \log \left(
          \frac{\exp(x_n \cdot \beta)}
               {\sum_{n' = n}^N \exp(x_{n'} \cdot \beta)}
       \right)
\\[4pt]
& = & x_n \cdot \beta - \log \sum_{n' = n}^N \exp(x_{n'} \cdot \beta)
\\
& = & x_n \cdot \beta - \textrm{logSumExp}_{n' = n}^N x_{n'} \cdot \beta.
\end{eqnarray*}

A simple normal prior on the components of $\beta$ completes the
model,
\[
\beta \sim \textrm{normal}(0, 2).
\]
This should be scaled based on knowledge of the predictors.  


### Stan program

To simplify the Stan program, the survival times for uncensored events
are sorted into decreasing order (unlike in the mathematical
presentation, where they were sorted into ascending order).  The
covariates for censored and uncensored observations are separated into
two matrices.

```stan
data {
  int<lower=0> K;          // num covariates

  int<lower=0> N;          // num uncensored obs
  vector[N] t;             // event time (non-strict decreasing)
  matrix[N, K] x;          // covariates for uncensored obs

  int N_c;                 // num censored obs
  real<lower=t[N]> t_c;    // censoring time
  matrix[N_c, K] x_c;      // covariates for censored obs
}
```

The parameters are just the coefficients.

```stan
parameters {
  vector[K] beta;          // slopes (no intercept)
}
```

The prior is a simple independent centered normal distribution on each
coefficient (the sampling statement is vectorized).

```stan
model {
  beta ~ normal(0, 2);
  ...
```

The log likelihood is implemented so as to minimize duplicated effort.
The first order of business is to calculate the linear predictors,
which is done separately for the subjects whose event time is observed
and those for which the event time is censored.

```stan
  vector[N] log_theta = x * beta;
  vector[N_c] log_theta_c = x_c * beta;
```  

These vectors are computed using efficient matrix-vector multiplies.
The log of exponential values of the
censored covariates times the coefficients is reused in the
denominator of each factor, which on the log scale, starts with the
log sum of exponentials of the censored items' linear predictors.

```stan
  real log_denom = log_sum_exp(log_theta_c);
```

Then, for each observed survival time, going backwards from the latest
to the earliest, the denominator can be incremented (which turns into
a log sum of exponentials on the log scale), and then the target is
updated with its likelihood contribution.

```stan
  for (n in 1:N) {
    log_denom = log_sum_exp(log_denom, log_theta[n]);
    target += log_theta[n] - log_denom;   // log likelihood
  }
```

The running log sum of exponentials is why the list is iterated in
reverse order of survival times.  It allows the log denominator to be
accumulated one term at a time.  The condition that the survival times
are sorted into decreasing order is not checked.  It could be checked
very easily in the transformed data block by adding the following
code.

```stan
transformed data {
  for (n in 2:N) {
    if (!(t[n] < t[n - 1])) {
      reject("times must be strictly decreasing, but found"
             "!(t[", n, "] < t[, ", (n - 1), "])");
    }   
  }
}
```

### Stan model for tied survival times

Technically, for continuous survival times, the probability of two
survival times being identical will be zero.  Nevertheless, real data
sets often round survival times, for instance to the nearest day or
week in a multi-year clinical trial.  The technically "correct" thing
to do in the face of unknown survival times in a range would be to
treat their order as unknown and infer it.  But considering all
$N!$ permutations for a set of $N$ subjects with tied survival times
is not tractable.  As an alternative, @efron:1972 introduced an
approximate partial likelihood that is better than a random
permutation while not being as good as considering all permutations.
The idea is to average the contributions as if they truly did occur
simultaneously.

In the interest of completeness, here is the Stan code for an
implementation of Efron's estimator.  It uses two user-defined
functions.  The first calculates how many different survival times
occur in the data.

```stan
functions {
  int num_unique_starts(vector t) {
    if (size(t) == 0) return 0;
    int us = 1;
    for (n in 2:size(t)) {
      if (t[n] != t[n - 1]) us += 1;
    }
    return us;
  }
```

This is then used to compute the value `J` to send into the function
that computes the position in the array of failure times where each
new failure time starts, plus an end point that goes one past the
target.  

```stan
  array[] int unique_starts(vector t, int J) {
    array[J + 1] int starts;
    if (J == 0) return starts;
    starts[1] = 1;
    int pos = 2;
    for (n in 2:size(t)) {
      if (t[n] != t[n - 1]) {
	starts[pos] = n;
	pos += 1;
      }
    }
    starts[J + 1] = size(t) + 1;
    return starts;
  }
}
```

The data format is exactly the same as for the model in the previous
section, but in this case, a transformed data blocks is used to
cache some precomputations required for the model.

```
transformed data {
  int<lower=0> J = num_unique_starts(t);
  array[J + 1] int<lower=0> starts = unique_starts(t, J);
}
```

The parameters and prior are also the same---just a vector `beta` of
coefficients with a centered normal prior.  Although it starts with
the same caching of results for later, and uses the same accumulator
for the denominator, the overall partial likelihood is
much more involved, and depends on the user-defined functions defining
`J` and `starts`.

```stan
  vector[N] log_theta = x * beta;
  vector[N_c] log_theta_c = x_c * beta;
  real log_denom_lhs = log_sum_exp(log_theta_c);
  for (j in 1:J) {
    int start = starts[j];
    int end = starts[j + 1] - 1;
    int len = end - start + 1;
    real log_len = log(len);
    real numerator = sum(log_theta[start:end]);
    log_denom_lhs = log_sum_exp(log_denom_lhs, log_sum_exp(log_theta[start:end]));
    vector[len] diff;
    for (ell in 1:len) {
      diff[ell] = log_diff_exp(log_denom_lhs,
                               log(ell - 1) - log_len
                                 + log_sum_exp(log_theta[start:end]));
    }
    target += numerator - sum(diff);
  }
```

The special function `log_diff_exp` is defined as
$\textrm{log\_diff\_exp}(u, v) = \log(\exp(u) - \exp(v)).$




