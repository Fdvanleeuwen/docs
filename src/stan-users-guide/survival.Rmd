# Survival Models {#survival-models.chapter}

Survival models arise when there is an event of interest for
a group of subjects that is

* certain to occur after some amount of time, 
* but only measured for a fixed period of time, during which the event
may not have occurred for all subjects. 

For example, one might wish to estimate the the distribution of time
to failure for solid state drives in a data center, but only measure
drives for a two year period, after which some number will have failed
and some will still be in service.

Survival models are often used comparatively, such as comparing time
to death of patients diagnosed with stage one liver cancer under a new
treatment and a standard treatment (pure controls are not allowed when
there is an effective existing treatment for a serious condition).
During a two year trial, some patients will die and others will survive.

Survival models may involve covariates, such as the factory at which a
component is manufactured, the day on which it is manufactured, and
the amount of usage it gets.  A clinical trial might be adjusted for
the sex and age of a cancer patient or the hospital at which treatment
is received.

Survival models come in two main flavors, parametric and
semi-parametric.  In a parametric model, the survival time of a
subject is modeled explicitly using a parametric probability
distribution.  There is a great deal of flexibility in how the
parametric probability distribution is constructed.  The sections
below consider exponential and Weibull distributed survival times.

Rather than explicitly modeling a parametric survival probability,
semi-parametric survival models instead model the relative effect on
survival of covariates.  The final sections of this chapter consider
the proportional hazards survival model.


## Exponential survival model

The exponential distribution is commonly used in survival models where
there is a constant risk of failure that does not go up the longer a
subject survives.  This is because the exponential distribution is
memoryless in sense that if $T \sim \textrm{exponential}(\lambda)$ for
some rate $\lambda > 0,$ then 
\[
\Pr[T > t] = \Pr[T > t + t' \mid T > t'].
\]
If component survival times are distributed exponentially, it means the
distribution of time to failure is the same no matter how long the
item has already survived.  This can be a reasonable assumption for
electronic components, but is not a reasonable model for animal survival.

The exponential survival model has a single parameter for the rate,
which assumes all subjects have the same distribution of failure time
(this assumption is relaxed in the next section by introducing
per-subject covariates).  With the rate parameterization, the expected
survival time for a component with survival time represented as the
random variable $T$ is
\[
\mathbb{E}[T \mid \lambda] = \frac{1}{\lambda}.
\]
The exponential distribution is sometimes parameterized in terms of a
scale (i.e., inverse rate) $\beta = 1 / \lambda$.

The data for a survival model consists of two components.  First,
there is a vector $t \in (0, \infty)^N$ of $N$ observed failure times.
Second, there is a censoring time $t^{\textrm{cens}}$ such that
failure times greater than $t^{\textrm{cens}}$ are not observed.  The
censoring time assumption imposes a constraint which 
requires $t_n < t^{\textrm{cens}}$ for all $n \in 1{:}N.$ For the
censored subjects, the only thing required in the model is their total
count, $N^\textrm{cens}$ (their covariates are also required for
models with covariates).

The model for the observed failure times is exponential, so that for
$n \in 1{:}N,$
\[
t_n \sim \textrm{exponential}(\lambda).
\]

The model for the censored failure times is also exponential.  All
that is known of a censored item is that its failure time is greater
than the censoring time, so each censored item contributes a factor to
the likelihood of 
\[
\Pr[T > t^{\textrm{cens}}] = 1 - F_T(t^{\textrm{cens}}),
\]
where $F_T$ is the cumulative distribution function (cdf) of survival
time $T$.  The function $1 - F_T(t)$ is the complementary
cumulative distribution function (ccdf), and it is used directly to
define the likelihood
\begin{eqnarray*}
p(t, t^{\textrm{cens}}, N^{\textrm{cens}} \mid \lambda)
& = & 
\prod_{n=1}^N \textrm{exponential}(t_n \mid \lambda)
\cdot
\prod_{n=1}^{N^{\textrm{cens}}}
\textrm{exponentialCCDF}(t^{\textrm{cens}} \mid \lambda)
\\ 
& = &
\prod_{n=1}^N \textrm{exponential}(t_n \mid \lambda)
\cdot
\textrm{exponentialCCDF}(t^{\textrm{cens}} \mid \lambda)^{N^{\textrm{cens}}}
\cdot 

\end{eqnarray*}

On the log scale, that's
\[
\log p(t, t^{\textrm{cens}}, N^{\textrm{cens}} \mid \lambda)
=
\sum_{n=1}^N \log \textrm{exponential}(t_n \mid \lambda)
+ N^{\textrm{cens}} \cdot \log \textrm{exponentialCCDF}(t^{\textrm{cens}} \mid \lambda).
\]

The model can be completed with a standard lognormal prior on
$\lambda,$ 
\[
\lambda \sim \textrm{lognormal}(0, 1),
\]
which is reasonable if failure times are in the range of 0.1 to 10
time units, because that's roughly the 95% central interval for
a variable distributed $\textrm{lognormal}(0, 1)$.  In general, the
range of the prior (and likelihood!) should be adjusted with prior knowledge of expected
failure times.


### Stan program

The data for a simple survival analysis without covariates can be
coded as follows.

```stan
data {
  int<lower=0> N;
  vector[N] t;
  int<lower=0> N_cens;
  real<lower=0> t_cens;
}
```

In this program, `N` is the number of uncensored observations and `t`
contains the times of the uncensored observations.  There are a
further `N_cens` items that are right censored at time `t_cens`.
Right censoring means that if the time to failure is greater than
`t_cens`, it is only observed that the part survived until time 
`t_cens`.


There is a single rate parameter, the inverse of which is the expected
time to failure.

```stan
parameters {
  real<lower=0> lambda;
}
```

The exponential survival model and the prior are coded directly using
vectorized sampling and ccdf statements.

```stan
model {
  t ~ exponential(lambda);
  target += N_cens * exponential_lccdf(t_cens | lambda);

  lambda ~ lognormal(0, 1);
}
```

The likelihood for observed failures is just the exponential
distribution with rate `lambda`.  The Stan code is vectorized,
modeling each entry of the vector `t` as a having an exponential
distribution with rate `lambda`. This likelihood could have been
written as

```stan
for (n in 1:N) {
  t[n] ~ exponential(lambda);
}
```

The log likelihood for censored items is the number of censored items
times the log complementary cumulative distribution function at the
censoring time of the exponential distribution with rate
`lambda`.  The log likelihoods for the censored events could have
been added to the target log density one at a time,

```stan
for (n in 1:N)
  target += exponential_lccdf(t_cens | lambda);
```

to define the same log density, but it is much more efficient
computationally to multiply by a constant than do a handful of
sequential additions.


## Weibull survival model

The Weibull distribution is a popular alternative to the exponential
distribution in cases where there is a decreasing probability of
survival as a subject gets older.  The Weibull distribution models
this by generalizing the exponential distribution to include a
power-law trend.

The Weibull distribution is parameterized by a scale $\lambda > 0$ and
a shape $\kappa > 0.$  For an outcome $t \geq 0$, the Weibull distribution's
probability density function is
\[
\textrm{Weibull}(t \mid \lambda, \kappa)
= \frac{\kappa}{\lambda}
  \cdot \left( \frac{t}{\lambda} \right)^{\kappa - 1}
  \cdot \exp\left(-\left(\frac{t}{\lambda}\right)^{\kappa}\right).
\]
In contrast, recall that the exponential distribution can be expressed
using a rate (inverse scale) parameter $\beta > 0$ with probability
density function 
\[
\textrm{exponential}(t \mid \beta) =
\beta
\cdot
\exp(-\beta \cdot t).
\]
When $\kappa = 1$ and  $\beta = \frac{1}{\lambda},$ the Weibull
reduces to an exponential,
\[
\textrm{exponential}(t \mid \beta)
=
\textrm{Weibull}(t \mid \frac{1}{\beta}, 1).
\]
In other words, the Weibull is a continuous expansion of the
exponential distribution.

If $T \sim \textrm{Weibull}(\lambda, \kappa),$ then the expected value
is
\[
\mathbb{E}[T] = \lambda \cdot \Gamma\left(1 + \frac{1}{\kappa}\right),
\]
where the $\Gamma$ function is the continuous completion of the
factorial function.  As $\kappa \rightarrow 0$ for a fixed $\lambda$
or as $\lambda \rightarrow \infty$ for a fixed $\kappa$, the expected
survival time goes to infinity.

There are three regimes of the Weibull distribution.

* $\kappa = 1.$  The Weibull distribution reduces to the exponential
  distribution, with a constant rate of failure over time.  

* $\kappa < 1.$  A subject is more likely to fail early.  When $\kappa
  < 1,$ the Weibull density approaches infinity as $t \rightarrow 0$
  so the mode of the density is zero.

* $\kappa > 1.$  Subjects are unlikely to fail early.  When $\kappa < 1,$
  the Weibull density approaches zero as $t \rightarrow 0,$ so the
  mode of the density is bounded away from zero.

### Stan program

With Stan, one can just swap the exponential density for the Weibull
density with the appropriate parameters and the model remains
essentially the same.  Recall the exponential model's parameters and
model block.

```stan
parameters {
  real<lower=0> lambda;
}
model {
  t ~ exponential(lambda);
  target += N_cens * exponential_lccdf(t_cens | lambda);

  lambda ~ lognormal(0, 1);
}
```

The Weibull model just uses the Weibull parameters.

```stan
parameters {
  real<lower=0> alpha;
  real<lower=0> kappa;
}
model {
  t ~ weibull(lambda, kappa);
  target += N_cens * weibull_lccdf(t_cens | alpha, kappa);

  alpha ~ lognormal(0, 1);
  kappa ~ lognormal(0, 1);
}
```

As usual, if more is known about expected survival times, `alpha` and
`kappa` should be given more informative priors.



## Survival with covariates

Suppose that for each of $n \in 1{:}N$ items observed, both censored
and uncensored, there is a covariate (row) vector $x_n \in
\mathbb{R}^K.$ For example, a clinical trial may include the age (or a
one-hot encoding of an age group) and the sex of a participant; an
electronic component might include a one-hot encoding of the factory
at which it was manufactured and a covariate for the load under which
it has been run.

Survival with covariates replaces what is essentially a simple
regression with only an intercept $\lambda$ with a generalized linear
model with a log link, where the rate for item $n$ is
\[
\lambda_n = \exp(x_n \cdot \beta),
\]
where $\beta \in \mathbb{R}^K$ is a $K$-vector of regression
coefficients.  Thus
\[
t_n \sim \textrm{exponential}(\lambda_n).
\]
The censored items have probability
\[
\Pr[n\textrm{-th censored}] =
\textrm{exponential\_ccdf}(t^{\textrm{cens}} \mid x^{\textrm{cens}}_n
\cdot \beta).
\]

The the covariates form an $N \times K$ data matrix, $x \in
\mathbb{R}^{N \times K}$. An intercept can be introduced by adding a
column of 1 values to $x$.

A Stan program for the exponential survival model with covariates is
as follows. 

```stan
data {
  int<lower=0> N;
  vector[N] t;
  int<lower=0> N_cens;
  real<lower=0> t_cens;
  int<lower=0> K;
  matrix[N, K] x;
  matrix[N_cens, K] x_cens;
}
parameters {
  vector[K] gamma;
}
model {
  gamma ~ normal(0, 2);

  t ~ exponential(exp(x * gamma));
  target += exponential_lccdf(t_cens | exp(x_cens * gamma));
}
```

Both the censored and uncensored likelihoods are vectorized, one in
terms of the log complementary cumulative distribution function and
one in terms of the exponential distribution.


## Hazard and survival functions

Suppose $T$ is a random variable representing a survival time, with a
smooth cumulative distribution function
\[
F_T(t) = \Pr[T \leq t],
\]
so that its probability density function is
\[
p_T(t) = \frac{\textrm{d}}{\textrm{d}t} F_T(t).
\]

The *survival function* $S(t)$ is the probability of surviving until
at least time $t$, which is just the complementary cumulative
distribution function (ccdf) of the survival random variable $T$,
\[
S(t) = 1 - F_T(t).
\]
The survival function appeared in the Stan model in the previous
section as the likelihood for items that did not fail during the
period of the experiment (i.e., the censored failure times for the
items that survived through the trial period).

The *hazard function* $h(t)$ is the instantaneous risk of not
surviving past time $t$ assuming survival until time $t$, which is
given by
\[
h(t) = \frac{p_T(t)}{S(t)} = \frac{p_T(t)}{1 - F_T(t)}.
\]
The *cumulative hazard function* $H(t)$ is defined to be the accumulated
hazard over time,
\[
H(t) = \int_0^t h(u) \, \textrm{d}u.
\]

The hazard function and survival function are related through the
differential equation
\begin{eqnarray*}
h(t) & = & -\frac{\textrm{d}}{\textrm{d}t} \log S(t).
\\[4pt]
& = & -\frac{1}{S(t)} \frac{\textrm{d}}{\textrm{d}t} S(t)
\\[4pt]
& = & \frac{1}{S(t)} \frac{\textrm{d}}{\textrm{d}t} -(1 - F_Y(t))
\\[4pt]
& = & \frac{1}{S(t)} \frac{\textrm{d}}{\textrm{d}t} (F_Y(t) - 1)
\\[4pt]
& = & \frac{1}{S(t)} \frac{\textrm{d}}{\textrm{d}t} F_Y(t)
\\[4pt]
& = & \frac{p_T(t)}{S(t)}.
\end{eqnarray*}

If $T \sim \textrm{exponential}(\lambda)$ has an exponential
distribution, then its hazard function is constant,
\begin{eqnarray*}
h(t \mid \lambda)
& = & \frac{p_T(t \mid \lambda)}{S(t \mid \lambda)}
\\[4pt]
& = & \frac{\textrm{exponential}(t \mid \lambda)}{1 - \textrm{exponentialCCDF}(t \mid \lambda)}
\\[4pt]
& = & \frac{\lambda \cdot \exp(-\lambda \cdot t)}
           {1 - (1 - \exp(-\lambda \cdot t))}
\\[4pt]
& = & \frac{\lambda \cdot \exp(-\lambda \cdot t)}
           {\exp(-\lambda \cdot t)}
\\[4pt]
& = & \lambda.
\end{eqnarray*}
The exponential distribution is the only distribution of survival
times with a constant hazard function.

If $T \sim \textrm{Weibull}(\alpha, \kappa),$ then its hazard function
is
\begin{eqnarray*}
h(t \mid \alpha, \kappa)
& = & \frac{p_T(t \mid \alpha, \kappa)}{S(t \mid \alpha, \kappa)}
\\[4pt]
& = & \frac{\textrm{Weibull}(t \mid \alpha, \kappa)}{1 - \textrm{WeibullCCDF}(t \mid \alpha, \kappa)}
\\[4pt]
& = & 
\frac{\frac{\kappa}{\alpha} \cdot \left( \frac{t}{\alpha} \right)^{\kappa - 1}
      \cdot \exp\left(-\left(\frac{t}{\alpha} \right)^\kappa\right)}
     {1 - \left(1 - 
       \exp\left(-\left(\frac{t}{\alpha}\right)^\kappa
     \right)\right)}      
& = &
\\[4pt]
& = & \frac{\kappa}{\alpha}
  \cdot
  \left( \frac{t}{\alpha} \right)^{\kappa - 1}
\\[4pt]
& = & \kappa \cdot \alpha^{-\kappa} \cdot t^{\kappa - 1}.
\end{eqnarray*}

If $\kappa = 1$ the hazard is constant over time (which also follows
from the fact that the Weibull distribution reduces to the exponential
distribution when $\kappa = 1$). When $\kappa > 1,$ the hazard grows as
time increases, and when $\kappa < 1,$ it decreases over time.


## Proportional hazards model

The exponential model is parametric in that is specifies an explicit
parametric form for the distribution of survival times.  @cox:1972
introduced a semi-parametric survival model specified directly in
terms of a hazard function $h(t)$ rather than in terms of a
distribution over survival times.  Cox's model is semi-parametric in
that it does not model the full hazard function, instead modeling only
the proportional differences in hazards among subjects.

Let $x_n \in \mathbb{R}^K$ be a (row) vector of covariates for subject
$n$ so that the full covariate data matrix is $x \in \mathbb{R}^{N \times
K}$.  In Cox's model, the hazard function for subject $n$ is defined
conditionally in terms of their covariates $x_n$ and the parameter vector
$\gamma \in \mathbb{R}^K$ as
\[
h(t \mid x_n, \beta) = h_0(t) \cdot \exp(x_n \cdot \gamma),
\]
where $h_0(t)$ is a shared baseline hazard function and $x_n \cdot
\gamma = \sum_{k=1}^K x_{n, k} \cdot \beta_k$ is a row vector-vector
product. 

In the semi-parametric, proportional hazards model, the baseline
hazard function $h_0(t)$ is not modeled.  This is why it is called
"semi-parametric."  Only the factor $\exp(x_n \cdot \gamma),$ which
determines how individual $n$ varies by a proportion from the baseline
hazard, is modeled.  This is why it's called "proportional hazards."


### Partial likelihood function

The proportional specification of the hazard function is insufficient
to generate random variates because the baseline hazard function
$h_0(t)$ is unknown.  On the other hand, the proportional
specification is sufficient to generate a partial likelihood that
accounts for the order of the survival times.

The hazard function $h(t \mid x_n, \beta) = h_0(t) \cdot \exp(x_n
\cdot \beta)$ for subject $n$ represents the instantaneous probability
that subject $n$ fails at time $t$ given that it has survived until
time $t.$ The probability that subject $n$ is the first to fail among
$N$ subjects is thus proportional to subject $n$'s hazard function,
\[
\Pr[n \textrm{ first to fail at time } t]
\propto h(t \mid x_n, \beta).
\]
Normalizing yields
\begin{eqnarray*}
\Pr[n \textrm{ first to fail at time } t]
& = &  \frac{h(t \mid x_n, \beta)}
            {\sum_{n' = 1}^N h(t \mid x_{n'}, \beta)}
\\[4pt]
& = &  \frac{h_0(t) \cdot \exp(x_n \cdot \beta)}
            {\sum_{n' = 1}^N h_0(t) \cdot \exp(x_{n'} \cdot \beta)}
\\[4pt]
& = &  \frac{\exp(x_n \cdot \beta)}
            {\sum_{n' = 1}^N \exp(x_{n'} \cdot \beta)}.
\end{eqnarray*}

Suppose there are $N$ subjects with strictly *ordered* survival times $t_1 <
t_2 < \cdots < t_N$ and covariate (row) vectors $x_1, \ldots, x_N$.
Let $t^{\textrm{cens}}$ be the (right) censoring time and let
$N^{\textrm{obs}}$ be the largest value of $n$ such that $t_n \leq
t^{\textrm{cens}}$.  This means $N^{\textrm{obs}}$ is the number of
subjects whose failure time was observed.  The ordering is for
convenient indexing and does not cause any loss of
generality---survival times can simply be sorted into the necessary
order.

With failure times sorted in decreasing order, the partial likelihood
for each observed subject $n \in 1{:}N^{\textrm{obs}}$ can be
expressed as
\[
\Pr[n \textrm{ first to fail among } n, n + 1, \ldots N]
= \frac{\exp(x_n \cdot \beta)}
       {\sum_{n' = n}^N \exp(x_{n'} \cdot \beta)}.
\]
The group of items for comparison and hence the summation is over all
items, including those with observed and censored failure times.

The partial likelihood, defined in this form by @breslow:1975, is just
the product of the partial likelihoods for the observed subjects
(i.e., excluding subjects whose failure time is censored).
\[
\Pr[\textrm{observed failures ordered } 1, \ldots, N^{\textrm{obs}} |
x, \beta]
= \prod_{n = 1}^{N^{\textrm{obs}}}
  \frac{\exp(x_n \cdot \beta)}
       {\sum_{n' = n}^N \exp(x_{n'} \cdot \beta)}.
\]
On the log scale,
\begin{eqnarray*}
\log \Pr[\textrm{observed failures ordered } 1, \ldots, N^{\textrm{obs}} |
x, \beta]
& = &
\sum_{n = 1}^{N^{\textrm{obs}}}
  \log \left(
          \frac{\exp(x_n \cdot \beta)}
               {\sum_{n' = n}^N \exp(x_{n'} \cdot \beta)}
       \right)
\\[4pt]
& = & x_n \cdot \beta - \log \sum_{n' = n}^N \exp(x_{n'} \cdot \beta)
\\
& = & x_n \cdot \beta - \textrm{logSumExp}_{n' = n}^N \ x_{n'} \cdot \beta,
\end{eqnarray*}
where
\[
\textrm{logSumExp}{n = a}^b x_n
= \log \sum_{n = a}^b \exp(x_n)
\]
is implemented so as to preserve numerical precision.

This likelihood follows the same approach to ranking as that developed
by @plackett:1975 for estimating the probability of the order of the
first few finishers in a horse race. 

A simple normal prior on the components of $\beta$ completes the
model,
\[
\beta \sim \textrm{normal}(0, 2).
\]
This should be scaled based on knowledge of the predictors.  


### Stan program

To simplify the Stan program, the survival times for uncensored events
are sorted into decreasing order (unlike in the mathematical
presentation, where they were sorted into ascending order).  The
covariates for censored and uncensored observations are separated into
two matrices.

```stan
data {
  int<lower=0> K;          // num covariates

  int<lower=0> N;          // num uncensored obs
  vector[N] t;             // event time (non-strict decreasing)
  matrix[N, K] x;          // covariates for uncensored obs

  int N_c;                 // num censored obs
  real<lower=t[N]> t_c;    // censoring time
  matrix[N_c, K] x_c;      // covariates for censored obs
}
```

The parameters are just the coefficients.

```stan
parameters {
  vector[K] beta;          // slopes (no intercept)
}
```

The prior is a simple independent centered normal distribution on each
element of the parameter vector, which is vectorized in the Stan code.

```stan
model {
  beta ~ normal(0, 2);
  ...
```

The log likelihood is implemented so as to minimize duplicated effort.
The first order of business is to calculate the linear predictors,
which is done separately for the subjects whose event time is observed
and those for which the event time is censored.

```stan
  vector[N] log_theta = x * beta;
  vector[N_c] log_theta_c = x_c * beta;
```  

These vectors are computed using efficient matrix-vector multiplies.
The log of exponential values of the
censored covariates times the coefficients is reused in the
denominator of each factor, which on the log scale, starts with the
log sum of exponentials of the censored items' linear predictors.

```stan
  real log_denom = log_sum_exp(log_theta_c);
```

Then, for each observed survival time, going backwards from the latest
to the earliest, the denominator can be incremented (which turns into
a log sum of exponentials on the log scale), and then the target is
updated with its likelihood contribution.

```stan
  for (n in 1:N) {
    log_denom = log_sum_exp(log_denom, log_theta[n]);
    target += log_theta[n] - log_denom;   // log likelihood
  }
```

The running log sum of exponentials is why the list is iterated in
reverse order of survival times.  It allows the log denominator to be
accumulated one term at a time.  The condition that the survival times
are sorted into decreasing order is not checked.  It could be checked
very easily in the transformed data block by adding the following
code.

```stan
transformed data {
  for (n in 2:N) {
    if (!(t[n] < t[n - 1])) {
      reject("times must be strictly decreasing, but found"
             "!(t[", n, "] < t[, ", (n - 1), "])");
    }   
  }
}
```

### Stan model for tied survival times

Technically, for continuous survival times, the probability of two
survival times being identical will be zero.  Nevertheless, real data
sets often round survival times, for instance to the nearest day or
week in a multi-year clinical trial.  The technically "correct" thing
to do in the face of unknown survival times in a range would be to
treat their order as unknown and infer it.  But considering all $N!$
permutations for a set of $N$ subjects with tied survival times is not
tractable.  As an alternative, @efron:1977 introduced an approximate
partial likelihood with better properties than a random permutation
while not being quite as good as considering all permutations.
Efron's model averages the contributions as if they truly did occur
simultaneously.  

In the interest of completeness, here is the Stan code for an
implementation of Efron's estimator.  It uses two user-defined
functions.  The first calculates how many different survival times
occur in the data.

```stan
functions {
  int num_unique_starts(vector t) {
    if (size(t) == 0) return 0;
    int us = 1;
    for (n in 2:size(t)) {
      if (t[n] != t[n - 1]) us += 1;
    }
    return us;
  }
```

This is then used to compute the value `J` to send into the function
that computes the position in the array of failure times where each
new failure time starts, plus an end point that goes one past the
target.  This is a standard way in Stan to code ragged arrays.

```stan
  array[] int unique_starts(vector t, int J) {
    array[J + 1] int starts;
    if (J == 0) return starts;
    starts[1] = 1;
    int pos = 2;
    for (n in 2:size(t)) {
      if (t[n] != t[n - 1]) {
	starts[pos] = n;
	pos += 1;
      }
    }
    starts[J + 1] = size(t) + 1;
    return starts;
  }
}
```

The data format is exactly the same as for the model in the previous
section, but in this case, the transformed data block is used to
cache some precomputations required for the model, namely the ragged
array grouping elements that share the same survival time.

```
transformed data {
  int<lower=0> J = num_unique_starts(t);
  array[J + 1] int<lower=0> starts = unique_starts(t, J);
}
```

For each unique survival time `j` in `1:J`, the subjects
indexed from `starts[j]` to `starts[j + 1] - 1` (inclusive) share the
same survival time.  The number of elements with surival time `j` is thus
`(starts[j + 1] - 1) - starts[j] + 1`, or just `starts[j + 1] - starts[j]`.

The parameters and prior are also the same---just a vector `beta` of
coefficients with a centered normal prior.  Although it starts with
the same caching of results for later, and uses the same accumulator
for the denominator, the overall partial likelihood is
much more involved, and depends on the user-defined functions defining
the transformed data variables `J` and `starts`.

```stan
  vector[N] log_theta = x * beta;
  vector[N_c] log_theta_c = x_c * beta;
  real log_denom_lhs = log_sum_exp(log_theta_c);
  for (j in 1:J) {
    int start = starts[j];
    int end = starts[j + 1] - 1;
    int len = end - start + 1;
    real log_len = log(len);
    real numerator = sum(log_theta[start:end]);
    log_denom_lhs = log_sum_exp(log_denom_lhs, log_sum_exp(log_theta[start:end]));
    vector[len] diff;
    for (ell in 1:len) {
      diff[ell] = log_diff_exp(log_denom_lhs,
                               log(ell - 1) - log_len
                                 + log_sum_exp(log_theta[start:end]));
    }
    target += numerator - sum(diff);
  }
```

The special function `log_diff_exp` is defined as

\[
\textrm{logDiffExp}(u, v) = \log(\exp(u) - \exp(v)).
\]

Because of how `J` and `starts` are constructed, the length `len` will
always be strictly positive so that the log is well defined.





