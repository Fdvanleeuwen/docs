# Pathfinder

The Pathfinder algorithm is a variational method for approximately
sampling from differentiable log densities.  Starting from a random
initialization, Pathfinder locates normal approximations to the target
density along a quasi-Newton optimization path, with local covariance
estimated using the inverse Hessian estimates produced by the
LBFGS optimizer.  Pathfinder returns draws from the Gaussian approximation
with the lowest estimated Kullback-Leibler (KL) divergence to the true
posterior.  Compared to ADVI and short dynamic HMC runs, Pathfinder
requires one to two orders of magnitude fewer log density and gradient
evaluations, with greater reductions for more challenging posteriors.

Stan provides two versions of the Pathfinder algorithm:  single-path Pathfinder and multi-path Pathfinder.
Single-path Pathfinder generates a set of approximate draws from one run of the basic Pathfinder algorithm.
Multi-path Pathfinder uses importance resampling over the draws from multiple runs of Pathfinder.
This better matches non-normal target densities and also mitigates
the problem of L-BFGS getting stuck at local optima or in saddle points on plateaus.
The Monte Carlo KL divergence estimates are embarrassingly parallelizable in the
core Pathfinder algorithm, as are multiple runs in the resampling
version, further increasing Pathfinder's speed advantage with multiple cores.

See: [Pathfinder: Parallel quasi-Newton variational inference](https://doi.org/10.48550/arXiv.2108.03782),
Lu Zhang, Bob Carpenter, Andrew Gelman, Aki Vehtari.
